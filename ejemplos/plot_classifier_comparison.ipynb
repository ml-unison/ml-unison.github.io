{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
        "\n",
        "# Comparación de modelos de clasificación\n",
        "\n",
        "## Aprendizaje Automático Aplicado\n",
        "\n",
        "## Maestría en Ciencia de Datos\n",
        "\n",
        "**Ivo Jiménez** y **Julio Waissman**, 2022\n",
        "\n",
        "[Abrir en google Colab](https://colab.research.google.com/github/mcd-unison/aaa-curso/blob/main/ejemplos/plot_classifier_comparison.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Libreta tomada de [la documentación de `sklearn`](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html). De acuerdo a los autores originales (Gaël Varoquaux, Andreas Müller, modificado por Jaques Grobler):\n",
        "\n",
        "\n",
        "> A comparison of a several classifiers in scikit-learn on synthetic datasets.\n",
        "> The point of this example is to illustrate the nature of decision boundaries\n",
        "> of different classifiers.\n",
        "> This should be taken with a grain of salt, as the intuition conveyed by\n",
        "> these examples does not necessarily carry over to real datasets.\n",
        "> \n",
        "> Particularly in high-dimensional spaces, data can more easily be separated\n",
        "> linearly and the simplicity of classifiers such as naive Bayes and linear SVMs\n",
        "> might lead to better generalization than is achieved by other classifiers.\n",
        "> \n",
        "> The plots show training points in solid colors and testing points\n",
        "> semi-transparent. The lower right shows the classification accuracy on the test\n",
        "> set.\n",
        "\n",
        "En nuestro caso, lo que nos parece más importante de resaltar es la API estandarizada que se utiliza en `sklearn`\n",
        "para diferentes modelos de aprendizaje. Vamos a dejar la misma libreta pero por partes para ir discutiendo que es lo que se hace. \n",
        "\n",
        "Vamos a iniciar cargando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code source: Gaël Varoquaux\n",
        "#              Andreas Müller\n",
        "# Modified for documentation by Jaques Grobler\n",
        "# License: BSD 3 clause\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Modulos generales\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Para dividir los conjuntos en conjunto de entrenamiento y prueba\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Para escalar los datos\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Para crear conjuntos de datos sintéticos\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y ahora vamos a cargar unos 9 modelos de ML para clasificación. Por el momento no importa si los conocemos o no, solo para ver como se pueden usar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "¿Que son? ¿Que hacen?, vamos a generar una lista de objetos que ya es un modelo particular, y otra lista con sus nombres\n",
        "(para efectos de presentación)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    SVC(gamma=2, C=1),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis(),\n",
        "]\n",
        "\n",
        "# Y ahora los nombres\n",
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Linear SVM\",\n",
        "    \"RBF SVM\",\n",
        "    \"Gaussian Process\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net\",\n",
        "    \"AdaBoost\",\n",
        "    \"Naive Bayes\",\n",
        "    \"QDA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Listo, ya tenemos 10 modelos de aprendizaje diferentes. Ahora nos faltan los datos en los que vamos a aplicar los modelos.\n",
        "\n",
        "Para los datos vamos a crear 3 conjuntos sintéticos en solo dos dimensiones para que podamos visualizar:\n",
        "\n",
        "1. Un conjunto usando un generador de lunas con dos clases (como el ying y el yang), a las que agregaremos ruido.\n",
        "2. Un conjunto *envuelto* en otro conjunto (circulos concentricos) pero con ruido\n",
        "3. Un conjunto linealmente separable, pero con ruido en los datos\n",
        "\n",
        "Vamos a generar entonces los conjuntos de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Datos sintéticos en forma de lunas\n",
        "Xm, ym = make_moons(noise=0.3, random_state=0)\n",
        "\n",
        "# Datos sintéticos en forma de circulos concentricos\n",
        "Xc, yc = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
        "\n",
        "# Datos sintéticos de un linealmente separable, mas ruido\n",
        "Xl, yl = make_classification(\n",
        "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
        ")\n",
        "rng = np.random.RandomState(2)\n",
        "Xl += 2 * rng.uniform(size=Xl.shape)\n",
        "\n",
        "datasets = [\n",
        "    (Xm, ym),\n",
        "    (Xc, yc),\n",
        "    (Xl, yl)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Y aqui va a suceder la mágia. Bueno esta parte del código no se puede separar por partes, así que la vamos a ir revisando juntos paso a paso. Lo más importante ees ver como hay una serie de métodos genericos que nos permiten usar mñodelos de aprendizaje sin siquiera saber de que se tratan o que es lo que hacen. \n",
        "\n",
        "Eso lo hace muy atractivo y al mismo tiempo muy peligroso, vamos a verlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h = 0.02  # step size in the mesh\n",
        "\n",
        "figure = plt.figure(figsize=(27, 9))\n",
        "i = 1\n",
        "# itera sobre los 3 conjuntos de datos\n",
        "for ds_cnt, ds in enumerate(datasets):\n",
        "    \n",
        "    # Estandariza los datos de entrada y separa en conjunto de entrenamiento y prueba\n",
        "    X, y = ds\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, random_state=42\n",
        "    )\n",
        "\n",
        "    # Calcula los valores minimo y máximo a clasificar\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "\n",
        "    # Genera datos para clasificar en toda la gráfica con incrementos de h\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.arange(x_min, x_max, h), \n",
        "        np.arange(y_min, y_max, h)\n",
        "    )\n",
        "\n",
        "    # Grafica el conjunto de datos\n",
        "    cm = plt.cm.RdBu\n",
        "    cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "    if ds_cnt == 0:\n",
        "        ax.set_title(\"Input data\")\n",
        "    # Plot the training points\n",
        "    ax.scatter(\n",
        "        X_train[:, 0], \n",
        "        X_train[:, 1], \n",
        "        c=y_train, \n",
        "        cmap=cm_bright, \n",
        "        edgecolors=\"k\"\n",
        "    )\n",
        "    # Plot the testing points\n",
        "    ax.scatter(\n",
        "        X_test[:, 0], \n",
        "        X_test[:, 1], \n",
        "        c=y_test, \n",
        "        cmap=cm_bright, \n",
        "        alpha=0.6, edgecolors=\"k\"\n",
        "    )\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    \n",
        "    # Y ahora listos para la siguiente gráfica\n",
        "    i += 1\n",
        "\n",
        "    # iterate over classifiers\n",
        "    for name, clf in zip(names, classifiers):\n",
        "\n",
        "        # Clasifica con datos de entrenamiento y calcula el score con los de prueba\n",
        "        clf.fit(X_train, y_train)\n",
        "        score = clf.score(X_test, y_test)\n",
        "\n",
        "        # Plot the decision boundary. For that, we will assign a color to each\n",
        "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "        if hasattr(clf, \"decision_function\"):\n",
        "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "        else:\n",
        "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "        Z = Z.reshape(xx.shape)\n",
        "\n",
        "        # Put the result into a color plot\n",
        "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
        "        ax.contourf(\n",
        "            xx, \n",
        "            yy, \n",
        "            Z, \n",
        "            cmap=cm, \n",
        "            alpha=0.8\n",
        "        )\n",
        "\n",
        "        # Plot the training points\n",
        "        ax.scatter(\n",
        "            X_train[:, 0], \n",
        "            X_train[:, 1], \n",
        "            c=y_train, \n",
        "            cmap=cm_bright, \n",
        "            edgecolors=\"k\"\n",
        "        )\n",
        "\n",
        "        # Plot the testing points\n",
        "        ax.scatter(\n",
        "            X_test[:, 0],\n",
        "            X_test[:, 1],\n",
        "            c=y_test,\n",
        "            cmap=cm_bright,\n",
        "            edgecolors=\"k\",\n",
        "            alpha=0.6,\n",
        "        )\n",
        "\n",
        "        ax.set_xlim(xx.min(), xx.max())\n",
        "        ax.set_ylim(yy.min(), yy.max())\n",
        "        ax.set_xticks(())\n",
        "        ax.set_yticks(())\n",
        "        if ds_cnt == 0:\n",
        "            ax.set_title(name)\n",
        "        ax.text(\n",
        "            xx.max() - 0.3,\n",
        "            yy.min() + 0.3,\n",
        "            (\"%.2f\" % score).lstrip(\"0\"),\n",
        "            size=15,\n",
        "            horizontalalignment=\"right\",\n",
        "        )\n",
        "        i += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
