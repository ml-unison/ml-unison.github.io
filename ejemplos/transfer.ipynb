{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg11ENFWoi6G"
      },
      "source": [
        "![](https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png)\n",
        "\n",
        "# Transferencia de aprendizaje: algunos ejemplos\n",
        "\n",
        "### Aprendizaje Automático Aplicado\n",
        "\n",
        "### Maestría en Ciencia de Datos\n",
        "\n",
        "**Julio Waissman**, 2022\n",
        "\n",
        "[Abrir en google Colab](https://colab.research.google.com/github/mcd-unison/aaa-curso/blob/main/ejemplos/transfer.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TD40kVJpun8"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "En esta libreta vamos a presentar una colección de aplicaciones que se pueden realizar utilizando modelos preentrenados. La mayoría de las aplicaciones provienen de [tensorFlow Hub](https://www.tensorflow.org/hub) para explotar modelos ya revisados y validados. \n",
        "\n",
        "Esta es la base para buscar otros modelos y otras aplicaciones posibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "C2SImRDn6yk7"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Copyright 2021 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRZPmBXSoGww"
      },
      "source": [
        "## 1. Tranferencia de estilo\n",
        "\n",
        "Tomado de [este ejemplo](https://www.tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization) y basado en el código del modelo en [magenta](https://github.com/tensorflow/magenta/tree/master/magenta/models/arbitrary_image_stylization) y la publicación:\n",
        "\n",
        "[Explorar la estructura de un tiempo real, la red arbitraria estilización artística neuronal](https://arxiv.org/abs/1705.06830) . *Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, Jonathon Shlens*, Actas de la Conferencia British Machine Vision (BMVC), 2017.\n",
        "\n",
        "Empecemos importando las librerías que vamos a necesitar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbgMhSkRqwaF"
      },
      "outputs": [],
      "source": [
        "# Para descargar las imagenes de estilo y contenido\n",
        "import functools\n",
        "import os\n",
        "\n",
        "# Para graficar las imágenes\n",
        "from matplotlib import gridspec\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Las que no pueden faltar\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Para obtener los modelos preentrenados\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Vamos viendo un poco de información\n",
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"TF Hub version: \", hub.__version__)\n",
        "print(\"Eager mode enabled: \", tf.executing_eagerly())\n",
        "print(\"GPU available: \", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-uOy1_8rVBW"
      },
      "source": [
        "Vamos primero a definir algunas funciones para descargar las imágenes y para visualizarlas. \n",
        "\n",
        "Es importante aqui hablar sobre decoradores en python, si es que no los conocen. En este caso el decorador (de las herramientas de programación funcional de python) es para agregar **memoización** a la función. Esto es, que si la función ya se ejecuto una vez con esas entradas, guadre la salida en una memoria cache, para no tener que recalcularlo cada vez.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nql0kPHwrUY3"
      },
      "outputs": [],
      "source": [
        "def crop_center(image):\n",
        "  \"\"\"\n",
        "  Recibe una imagen de (3, n, m) y regresa\n",
        "  una imagen cuadrada de (3, min(n,m), min(n,m))\n",
        "  centrada\n",
        "  \n",
        "  \"\"\"\n",
        "  shape = image.shape\n",
        "  new_shape = min(shape[1], shape[2])\n",
        "  offset_y = max(shape[1] - shape[2], 0) // 2\n",
        "  offset_x = max(shape[2] - shape[1], 0) // 2\n",
        "  \n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_y, offset_x, new_shape, new_shape\n",
        "  )\n",
        "  \n",
        "  return image\n",
        "\n",
        "def show_n(images, titles=('',)):\n",
        "  \"\"\"\n",
        "  Muestra una serie de imagenes en forma horizontal\n",
        "  y les agrega los títulos correspondientes\n",
        "\n",
        "  \"\"\"\n",
        "  n = len(images)\n",
        "  image_sizes = [image.shape[1] for image in images]\n",
        "  w = (image_sizes[0] * 6) // 320\n",
        "  plt.figure(figsize=(w * n, w))\n",
        "  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)\n",
        "  for i in range(n):\n",
        "    plt.subplot(gs[i])\n",
        "    plt.imshow(images[i][0], aspect='equal')\n",
        "    plt.axis('off')\n",
        "    plt.title(titles[i] if len(titles) > i else '')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "@functools.lru_cache(maxsize=None)\n",
        "def load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):\n",
        "  \"\"\"\n",
        "  Carga y procesa una imagen. \n",
        "\n",
        "  Usa memoización (@functools.lru_cache) para no descargar la misma imagen muchas veces\n",
        "  \n",
        "  \"\"\"\n",
        "  # Descarga el archivo de la imagen en crudo\n",
        "  image_path = tf.keras.utils.get_file(\n",
        "      os.path.basename(image_url)[-128:], image_url\n",
        "  )\n",
        "\n",
        "  # Carga y procesa la imagen\n",
        "  img = tf.io.decode_image(\n",
        "      tf.io.read_file(image_path),\n",
        "      channels=3, dtype=tf.float32\n",
        "  )[tf.newaxis, ...] \n",
        "\n",
        "  # Recorta y escala la imagen \n",
        "  img = crop_center(img)\n",
        "  img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n",
        "  \n",
        "  return img\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR01aUgvvajv"
      },
      "source": [
        "Vamos ahora a cargar unas imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "S9pedlwqvdPL",
        "outputId": "9c018857-b792-4bf5-b5bb-595300d847c4"
      },
      "outputs": [],
      "source": [
        "content_image_url = 'https://proyectopuente.com.mx/wp-content/uploads/2021/03/Unison-2.jpg'\n",
        "#style_image_url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTvjSkzPQVNjS5XOWmOwTdOdY_PaXB4T9KNW0cJIw3LIsL0zCZuhApTOfu7D0yD8dj_PhI&usqp=CAU'  \n",
        "style_image_url = 'https://media.admagazine.com/photos/618a6b39b67f79aa891edc46/master/w_1600%2Cc_limit/63307.jpg'\n",
        "output_image_size = 384 \n",
        "\n",
        "# La imagen con el contenido, puede ser de un tamaño arbitrario.\n",
        "content_img_size = (output_image_size, output_image_size)\n",
        "\n",
        "# La imagen con el estilo. Puede ser de cualquier tamaño pero el modelo que vamos a usar\n",
        "# está optimizado para imagenes de 256 x 256\n",
        "style_img_size = (256, 256)  # Recomendado mantener en 256.\n",
        "\n",
        "content_image = load_image(content_image_url, content_img_size)\n",
        "style_image = load_image(style_image_url, style_img_size)\n",
        "style_image = tf.nn.avg_pool(style_image, ksize=[3,3], strides=[1,1], padding='SAME')\n",
        "show_n([content_image, style_image], ['Content image', 'Style image'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIxYZlAymCO"
      },
      "source": [
        "Y ahora vamos a cargar el modelo preentrenado, el cual podemos revisar en la [página de TensorFlow Hub de este modelo](https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14GxuDIDy-dN"
      },
      "outputs": [],
      "source": [
        "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
        "hub_module = hub.load(hub_handle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAOtTdBvzr8Y"
      },
      "source": [
        "Y simplemente lo ejecutamos y vemos el resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch6HLfpUzvFf"
      },
      "outputs": [],
      "source": [
        "# Stylize content image with given style image.\n",
        "# This is pretty fast within a few milliseconds on a GPU.\n",
        "\n",
        "outputs = hub_module(\n",
        "    tf.constant(content_image), \n",
        "    tf.constant(style_image)\n",
        ")\n",
        "stylized_image = outputs[0]\n",
        "\n",
        "# Visualize input images and the generated stylized image.\n",
        "\n",
        "show_n(\n",
        "    [content_image, style_image, stylized_image], \n",
        "    titles=['Original content image', 'Style image', 'Stylized image']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOyTxJWA7XNG"
      },
      "source": [
        "## 2. Reentrenamiento de un clasificador de imágenes\n",
        "\n",
        "Tomado de [este ejemplo](https://www.tensorflow.org/hub/tutorials/tf2_image_retraining). \n",
        "\n",
        "Esta sección muestra cómo reutilizar un modelo existente  para clasificar cinco especies de flores. Se va a utilizar un modelo entrenado en el conjunto de datos ImageNet y guardado en TensorFlow Hub. Se van a mantener las capas de extracción de características y unicamente se va a modificar el clasificador. Opcionalmente, el extractor de características se puede entrenar (\"ajustar\") junto con el clasificador recién agregado.\n",
        "\n",
        "Vamos primero cargando las librerias necesarias (aún las ya cargadas previamente, solo para facilitar el copiado/pegado en otro problema). De hecho `itertools` es el único módulo que no habíamos cargado previamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNOINbrV82EA"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1osysHyZ9EQA"
      },
      "source": [
        "Vamos ahora a seleccionar un modelo a descargar. Para esto vamos a poner una lista (tomada obviamente del tutorial desarrollado por el equipo de TensorFlow Hub) de los modelos disponibles, así como el tamaño de las imágenes que usa cada uno de los modelos. Los vamos a guardar en un diccionario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LwmF9Lm9lYy"
      },
      "outputs": [],
      "source": [
        "model_handle_map = {\n",
        "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
        "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
        "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
        "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
        "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
        "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
        "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
        "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
        "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
        "}\n",
        "\n",
        "model_image_size_map = {\n",
        "  \"efficientnetv2-s\": 384,\n",
        "  \"efficientnetv2-m\": 480,\n",
        "  \"efficientnetv2-l\": 480,\n",
        "  \"efficientnetv2-b0\": 224,\n",
        "  \"efficientnetv2-b1\": 240,\n",
        "  \"efficientnetv2-b2\": 260,\n",
        "  \"efficientnetv2-b3\": 300,\n",
        "  \"efficientnetv2-s-21k\": 384,\n",
        "  \"efficientnetv2-m-21k\": 480,\n",
        "  \"efficientnetv2-l-21k\": 480,\n",
        "  \"efficientnetv2-xl-21k\": 512,\n",
        "  \"efficientnetv2-b0-21k\": 224,\n",
        "  \"efficientnetv2-b1-21k\": 240,\n",
        "  \"efficientnetv2-b2-21k\": 260,\n",
        "  \"efficientnetv2-b3-21k\": 300,\n",
        "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
        "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
        "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
        "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
        "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
        "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
        "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
        "  \"efficientnet_b0\": 224,\n",
        "  \"efficientnet_b1\": 240,\n",
        "  \"efficientnet_b2\": 260,\n",
        "  \"efficientnet_b3\": 300,\n",
        "  \"efficientnet_b4\": 380,\n",
        "  \"efficientnet_b5\": 456,\n",
        "  \"efficientnet_b6\": 528,\n",
        "  \"efficientnet_b7\": 600,\n",
        "  \"inception_v3\": 299,\n",
        "  \"inception_resnet_v2\": 299,\n",
        "  \"nasnet_large\": 331,\n",
        "  \"pnasnet_large\": 331,\n",
        "}\n",
        "\n",
        "print('Los modelos que podemos escoger son los siguientes:')\n",
        "\n",
        "print('\\n'.join(model_handle_map.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UOPwNRo9m_o"
      },
      "source": [
        "Y ahora si, seleccionamos nuestro modelo, el número de pixeles que maneja de imágenes de entrada y el tamaño del minibatch para reentrenar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6o6p4ya9zw1"
      },
      "outputs": [],
      "source": [
        "model_name = \"efficientnetv2-xl-21k\" \n",
        "\n",
        "model_handle = model_handle_map.get(model_name)\n",
        "pixels = model_image_size_map.get(model_name, 224)\n",
        "\n",
        "print(f\"Modelo seleccionado: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Tamaño de la imagen: {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SogJv-5R_saN"
      },
      "source": [
        "Y ahora vamos a cargar el conjunto de imágenes a clasificar (lo que haríamops en cualquier problema propio que tuvieramos que solucionar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19lmXDah_rfo"
      },
      "outputs": [],
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCLo87tgat7e"
      },
      "source": [
        "Ahora vamos a procesar los datos, y para eso vamos a usar el [modulo ``tf.data`](https://www.tensorflow.org/guide/data?hl=en) que permite crear *pipelines* para consumir (potencialmente) grandes cantidades de datos.\n",
        "\n",
        "Vamos a seleccionar un conjunto de entrenamiento con el 80% de los datos. Se separan los valores de salida y se generan minibatches de tamaño `BATCH_SIZE``. Se utiliza `tf.data.repeat()` para asegurar que se provea en forma indefinida de minibatches para el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz_GbReubwE5"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=.20,\n",
        "  subset=\"training\",\n",
        "  label_mode=\"categorical\",\n",
        "  # Seed needs to provided when using validation_split and shuffle = True.\n",
        "  # A fixed seed is used so that the validation set is stable across runs.\n",
        "  seed=123,\n",
        "  image_size=IMAGE_SIZE,\n",
        "  # Un dato por batch solo para sacar el tamaño de la muestra\n",
        "  batch_size=1 \n",
        ")\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "\n",
        "class_names = tuple(train_ds.class_names)\n",
        "\n",
        "# Ahora si, hacenmos batches de tamaño BATCH_SIZE\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "\n",
        "train_ds = train_ds.repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtZ5I5FXg4gb"
      },
      "source": [
        "Vamos a hacer una serie de pasos para el entrenamiento.\n",
        "\n",
        "1. Vamos a normalizar los datos entre 0 y 1\n",
        "2. Si decidimos hacer aumentación de datos para el entrenamiento, entonces:\n",
        "  1. Giro aleatorio entre -40 y 40 grados\n",
        "  2. Translación aleatoria vertical entre 0 y 20%\n",
        "  3. Translación aleatoria horizontal entre 0 y 20%\n",
        "  4. Zoom aleatorio entre 0 y 20%\n",
        "  5. Flip horizontal aleatorio\n",
        "\n",
        "Todo el preprocesamiento se hace como un modelo neuronal que se integra en el *pipeline* de aprendizaje\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYmHh7ImhEqQ"
      },
      "outputs": [],
      "source": [
        "do_data_augmentation = False\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "\n",
        "if do_data_augmentation:\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomRotation(.4))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
        "\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RkfKgSgjL_c"
      },
      "source": [
        "Ahora vamos a hacer el pipeline de validación, con los datos que se quedaron para validación (es necesario mantener una semilla fija para reproducibilidad)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgUtCY2uja1y"
      },
      "outputs": [],
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=.20,\n",
        "  subset=\"validation\",\n",
        "  label_mode=\"categorical\",\n",
        "  # Seed needs to provided when using validation_split and shuffle = True.\n",
        "  # A fixed seed is used so that the validation set is stable across runs.\n",
        "  seed=123,\n",
        "  image_size=IMAGE_SIZE,\n",
        "  batch_size=1\n",
        ")\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (normalization_layer(images), labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXuPpNXxlN5I"
      },
      "source": [
        "Vamos ahora a descargar el modelo seleccionado como capas internas de `Keras` de un modelo propio, donde se elimina la última capa para integrar la capa propia que vamos a hacer de nuestro clasificador.\n",
        "\n",
        "Si quisieramos entrenar, además de la última capa, los pesos de las capas del modelo descargado (lo que se conoce como *fine tunning*) sólo hay que cambiar el valor de la variable booleana, lo que permitira modificar capas internas del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlJWamoYl3bw"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = False\n",
        "\n",
        "print(\"Modelo basado en \", model_handle)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  # Define explicitamente el input shape \n",
        "  tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "  # Carga el modelo preentrenado, con o sin fine tunning\n",
        "  hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "  # Dropout para ayudar a la generalización\n",
        "  tf.keras.layers.Dropout(rate=0.2),\n",
        "  # Capa de salida lineal del número de clases y con regularización\n",
        "  tf.keras.layers.Dense(\n",
        "      len(class_names),\n",
        "      kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
        "  )\n",
        "], name='Mi-modelo-bien-bonito')\n",
        "\n",
        "# Construye el modelo haciendo explicito el tamaño de entrada\n",
        "model.build((None,) + IMAGE_SIZE + (3,))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k_61p4neD6"
      },
      "source": [
        "Ahora compilamos y decidimos que optimizador, función de pérdida y métricas utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJFwEW_onlvr"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(\n",
        "      learning_rate=0.005, \n",
        "      momentum=0.9\n",
        "  ), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, \n",
        "      label_smoothing=0.1\n",
        "  ),\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yRNMA7Qn1M7"
      },
      "source": [
        "... y entrenamos (y guardamos la evolución en `hist`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ukR9zSan3oS"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=5, \n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps\n",
        ").history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgfXjwPwo1Rz"
      },
      "source": [
        "Y ahora grafiquemos la función de pérdida y la métrica seleccionada por los *epochs* que escogimos simular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bb8Hc_wUo_8g"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Funcion de pérdida\")\n",
        "plt.xlabel(\"Pasos de aprendizaje\")\n",
        "#plt.ylim([0,1])\n",
        "plt.plot(hist[\"loss\"], label='Entrenamiento')\n",
        "plt.plot(hist[\"val_loss\"], label='Validación')\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"% de error\")\n",
        "plt.xlabel(\"Pasos de aprendizaje\")\n",
        "#plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"], label='Entrenamiento')\n",
        "plt.plot(hist[\"val_accuracy\"], label='Validación')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U7pYC9XpJkX"
      },
      "source": [
        "Probemos como se enviaría una imagen nueva al clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mwtKkAlpVo9"
      },
      "outputs": [],
      "source": [
        "# Con el iterador se obtiene un minibatch de imágenes y clases\n",
        "x, y = next(iter(val_ds)) \n",
        "\n",
        "# Tomemos la primer imagen y su indice\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "\n",
        "# Mostramos la imagen\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expande la imagen a (1, 224, 224, 3) para introducirla al modelo\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "\n",
        "# Obten el indice con mayor valor en la capa de salida\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "\n",
        "print(\"True label: \" + class_names[true_index])\n",
        "print(\"Predicted label: \" + class_names[predicted_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nffMZv5vq8D6"
      },
      "source": [
        "Y si el modelo te gustó y lo quieres usar más adelate, sólamente hay que guardarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUdvAJ0GrDa2"
      },
      "outputs": [],
      "source": [
        "saved_model_path = f\"/tmp/saved_flowers_model_{model_name}\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z4XS9MxzVG9"
      },
      "source": [
        "## 3. Detección de objetos con modelos preentrenados\n",
        "\n",
        "Vamos a explorar como realizar la detección de objetos usando los modelos preentrenados disponibles en TensorFlow Hub. Esta es obtenida a partir de [esta libreta de colab](https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb).\n",
        "\n",
        "Iniciamos cargando las librerías, aunque ya estén cargadas anteriormente, para facilitar el copiado y pegado en otros proyectos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JFpF7fj0BVK"
      },
      "outputs": [],
      "source": [
        "# Las imprecindibles\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Para mostrar las imágenes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Para descargar las imágenes\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "\n",
        "# Para dibujar cuadritos en las imágenes.\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Para medir el tiempo que toma.\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adgYFG5F0hWv"
      },
      "source": [
        "Y ahora vamos a hacer unas funciones simples para editar y visualizar las imágenes con el reconocimiento de objetos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xSP9Eta1Bix"
      },
      "outputs": [],
      "source": [
        "def display_image(image):\n",
        "  \"\"\"\n",
        "  Muestra una imagen en pantalla\n",
        "\n",
        "  \"\"\"\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "\n",
        "\n",
        "def download_and_resize_image(url, new_width=256, new_height=256, \n",
        "                              display=False):\n",
        "  \"\"\"\n",
        "  Descarga y reescala una imagen y la guarda como jpg\n",
        "\n",
        "  \"\"\"\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  \n",
        "  pil_image = Image.open(image_data)\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  \n",
        "  print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename\n",
        "\n",
        "\n",
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax,xmax,\n",
        "                               color, thickness=4, display_str_list=()):\n",
        "  \"\"\"\n",
        "  Agrega un bounding box a una imagen con una etiqueta\n",
        "\n",
        "  ymin, xmin, ymax, xmax \\in [0, 1] posiciones en la imagen\n",
        "  \n",
        "  \"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  font = ImageFont.load_default()\n",
        "  \n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  \n",
        "  draw.line(\n",
        "      [(left, top), (left, bottom), (right, bottom), (right, top), (left, top)],\n",
        "      width=thickness,\n",
        "      fill=color\n",
        "  )\n",
        "\n",
        "  # If the total height of the display strings added to the top of the bounding\n",
        "  # box exceeds the top of the image, stack the strings below the bounding box\n",
        "  # instead of above.\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  \n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  \n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle(\n",
        "        [(left, text_bottom - text_height - 2 * margin), (left + text_width, text_bottom)],\n",
        "        fill=color\n",
        "    )\n",
        "    draw.text(\n",
        "        (left + margin, text_bottom - text_height - margin),\n",
        "        display_str,\n",
        "        fill=\"black\", font=font\n",
        "    )\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "\n",
        "  for i in range(min(boxes.shape[0], max_boxes)):\n",
        "    if scores[i] >= min_score:\n",
        "      ymin, xmin, ymax, xmax = tuple(boxes[i])\n",
        "      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n",
        "                                     int(100 * scores[i]))\n",
        "      color = colors[hash(class_names[i]) % len(colors)]\n",
        "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "      draw_bounding_box_on_image(\n",
        "          image_pil, ymin, xmin, ymax, xmax,\n",
        "          color, display_str_list=[display_str]\n",
        "      )\n",
        "      np.copyto(image, np.array(image_pil))\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdVlnT2j4ax6"
      },
      "source": [
        "Ahora vamos a descargar alguna imagen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5OHrxMu42j6"
      },
      "outputs": [],
      "source": [
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg\"  \n",
        "downloaded_image_path = download_and_resize_image(image_url, 1280, 856, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETFXgBwJ7SRd"
      },
      "source": [
        "Ahora vamos a seleccionar uno de los dos modelos más sencillos para detección de objetos:\n",
        "\n",
        "- `FasterRCNN+InceptionResNet V2`: Alta presición,\n",
        "- `ssd+mobilenet V2`: pequeño y rápido.\n",
        "\n",
        "Se agrega una funcion para utilizar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngu7sHhd7q_C"
      },
      "outputs": [],
      "source": [
        "module_handle_map ={\n",
        "  \"FasterRCNN\": \"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\", \n",
        "  \"ssd\": \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n",
        "}\n",
        "\n",
        "module_handle = module_handle_map[\"ssd\"]\n",
        "detector = hub.load(module_handle).signatures['default']\n",
        "\n",
        "def run_detector(detector, path):\n",
        "\n",
        "  # Lee una imagen y la pasa a formato de tensorflow\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n",
        "  \n",
        "  # Esta es la detección de objetos\n",
        "  start_time = time.time()\n",
        "  result = detector(converted_img)\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Convertimos todos los resultados (un diccionario de valores)\n",
        "  # a objetos numpy, y no de tf\n",
        "  result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "  # ¿Que encontramos?\n",
        "  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n",
        "  print(\"Inference time: \", end_time-start_time)\n",
        "\n",
        "  # Agregale las bbox a la imagen\n",
        "  image_with_boxes = draw_boxes(\n",
        "      img.numpy(), result[\"detection_boxes\"],\n",
        "      result[\"detection_class_entities\"], result[\"detection_scores\"])\n",
        "\n",
        "  # Muestra la imagen\n",
        "  display_image(image_with_boxes)\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnFIDCvO8lLS"
      },
      "source": [
        "Y ahora si, probamos con la imagen que acabamos de descargar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kiwTXmb-RK3"
      },
      "outputs": [],
      "source": [
        "results = run_detector(detector, downloaded_image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7-ivNwN_ITq"
      },
      "source": [
        "Probemos con otras imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfXHK2zW_LMW"
      },
      "outputs": [],
      "source": [
        "image_url = \"http://sic.gob.mx/galeria_imagen/5ec45420024d4sala_publicaciones.jpg\"  \n",
        "downloaded_image_path = download_and_resize_image(image_url, 933, 700, False)\n",
        "result = run_detector(detector, downloaded_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUoEV-uIC3YG"
      },
      "outputs": [],
      "source": [
        "result[\"detection_class_entities\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMFtJZpxDYGR"
      },
      "outputs": [],
      "source": [
        "result[\"detection_scores\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "transfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
