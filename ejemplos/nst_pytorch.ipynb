{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkjLrsQamWJ5"
      },
      "source": [
        "# *Neural Style Transfer* con pyTorch\n",
        "\n",
        "## Reconocimiento de patrones\n",
        "\n",
        "### Licenciatura en Ciencias de la Computación\n",
        "\n",
        "#### **Julio Waissman**, 2024\n",
        "\n",
        "[**Abrir en google Colab**](https://colab.research.google.com/github/ml-unison/ml-unison-github.io/blob/main/ejemplos/nst_pytorch.ipynb)\n",
        "\n",
        "\n",
        "Esta libreta está enteramente tomado de [este tutorial de pyTorch](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cad5020cab595c3bf83a518b7e4d4125/neural_style_tutorial.ipynb) pero quité algunas explicaciones y agregué otras que me hubiera gustado tener a mi. Puedes ejecutar cualquiera de las libretas (la original o esta), asumiendo que la otra está mejor explicada. Te recomiendo revisar esta presentación sobre [transferencia del aprendizaje](https://github.com/ml-unison/ml-unison.github.io/raw/main/slides/transfer_learning.pptx) (tambien [en pdf](https://github.com/ml-unison/ml-unison.github.io/raw/main/slides/transfer_learning.pdf)) ya que vamos a implementar, paso a paso, el algoritmo que se describe.\n",
        "\n",
        "Si quieres usar solamente el NST, entonces te recomiendo le pegues un ojo a [esta entrada de Medium](https://medium.com/geekculture/a-lightweight-pytorch-implementation-of-neural-style-transfer-86603e5eb551)\n",
        "\n",
        "\n",
        "Vamos entonces cargando modulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "55l0EVCJmSpc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "\n",
        "import copy\n",
        "\n",
        "# Para esto vamos a necesitar GPU o no acabamos nunca\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.set_default_device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5apOlCHnkML"
      },
      "source": [
        "## 1. Cargando las imágenes y visualizandolas\n",
        "\n",
        "Aqui vamos a bajar una imagen de contenido (**C**) y una de estilo (**S**), que vamos a usar para combinarlas en una nueva (hay que ajustar su tamaño a lo que usa el modelo [VGG19](https://arxiv.org/abs/1409.1556)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKgWYJXRoTq-",
        "outputId": "fd5e1da8-3a9e-4ac5-e1a5-03a49dc27e4f"
      },
      "outputs": [],
      "source": [
        "!curl https://raw.githubusercontent.com/nazianafis/Neural-Style-Transfer/main/data/content-images/c1.jpg -o content.jpg\n",
        "!curl https://raw.githubusercontent.com/nazianafis/Neural-Style-Transfer/main/data/style-images/s1.jpg -o style.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "lCls0mddpClv"
      },
      "outputs": [],
      "source": [
        "imsize = 512 if torch.cuda.is_available() else 128  # use small size if no GPU\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize(imsize),  # scale imported image\n",
        "    transforms.CenterCrop(imsize),\n",
        "    transforms.ToTensor()])  # transform it into a torch tensor\n",
        "\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    # fake batch dimension required to fit network's input dimensions\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "\n",
        "style_img = image_loader(\"./style.jpg\")\n",
        "content_img = image_loader(\"./content.jpg\")\n",
        "\n",
        "assert style_img.size() == content_img.size(), \\\n",
        "    \"we need to import style and content images of the same size\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT8HM0fLsnoA"
      },
      "source": [
        "Y vamos a visualizar las imagenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "HLtWS6_xsq2-",
        "outputId": "53a44320-2429-4ef8-dad3-e2c13b406abb"
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
        "\n",
        "def imshow(tensor, title=None):\n",
        "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
        "    image = image.squeeze(0)      # remove the fake batch dimension\n",
        "    image = unloader(image)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1,2,1)\n",
        "imshow(content_img, title='Content Image')\n",
        "plt.subplot(1,2,2)\n",
        "imshow(style_img, title='Style Image')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0to9Wx3utydr"
      },
      "source": [
        "## 2. Funciones de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLx3d-V8aWh"
      },
      "source": [
        "\n",
        "### Funcion de pérdida de contenido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "X9DUyaght9zP"
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target,):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        # we 'detach' the target content from the tree used\n",
        "        # to dynamically compute the gradient: this is a stated value,\n",
        "        # not a variable. Otherwise the forward method of the criterion\n",
        "        # will throw an error.\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKsmzDdSuQij"
      },
      "source": [
        "**Nota del equipo de pyTorch**:\n",
        "\n",
        "> Although this module is named <code>ContentLoss</code>, itis not a true\n",
        "> PyTorch Loss function. If you want to define your contentloss as a\n",
        "> PyTorch Loss function, you have to create a PyTorch autograd function\n",
        "> to recompute/implement the gradient manually in the `backward method`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTKewmzAuvlM"
      },
      "source": [
        "### Funcion de pérdida de estilo\n",
        "\n",
        "Este es un poco más complicada ya que se mide por la covarianza de las matrices de Gram de cada capa, así que primero hacemos una funcion para calgular las matrices y luego otra que calcula la pérdida.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6CA8cACGvHRd"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()\n",
        "    # a=batch size(=1)\n",
        "    # b=number of feature maps\n",
        "    # (c,d)=dimensions of a f. map (N=c*d)\n",
        "\n",
        "    features = input.view(a * b, c * d)  # resize F_XL into \\hat F_XL\n",
        "\n",
        "    G = torch.mm(features, features.t())  # compute the gram product\n",
        "\n",
        "    # we 'normalize' the values of the gram matrix\n",
        "    # by dividing by the number of element in each feature maps.\n",
        "    return G.div(a * b * c * d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "XnEjdvlmvi4f"
      },
      "outputs": [],
      "source": [
        "class StyleLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = gram_matrix(input)\n",
        "        self.loss = F.mse_loss(G, self.target)\n",
        "        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9qE8EO4vnv8"
      },
      "source": [
        "## 3. Modelo de evaluación de imagen de salida\n",
        "\n",
        "Vamos primero a bajar el modelo VGG19. Fijense que el modelo lo cargamos solo para evaluación, por lo que sus pesos no se van a modificar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTBj2UVgvtjg",
        "outputId": "3e4f63a3-a7c8-418d-87e4-6205fec19254"
      },
      "outputs": [],
      "source": [
        "cnn = vgg19(weights=VGG19_Weights.DEFAULT).features.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIBH8wylwCCU"
      },
      "source": [
        "Si revisamos la [documentación del modelo VGG19 en torchvision](https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg19.html#torchvision.models.vgg19), dice esto:\n",
        "\n",
        "\n",
        "> The inference transforms are available at VGG19_Weights.IMAGENET1K_V1.transforms and\n",
        "> perform the following preprocessing operations: \n",
        "> \n",
        "> Accepts PIL.Image, batched (B, C, H, W) and\n",
        "> single (C, H, W) image torch.Tensor objects. \n",
        "> \n",
        "> The images are resized to resize_size=[256]\n",
        "> using interpolation=InterpolationMode.BILINEAR, followed by a central crop of \n",
        "> `crop_size=[224]`. \n",
        "> \n",
        "> Finally the values are first rescaled to [0.0, 1.0] and then normalized \n",
        "> using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225].\n",
        "\n",
        "Por lo que tenemos que recortar y normalizar las imágenes para usar este modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "68foMXKAwxhM"
      },
      "outputs": [],
      "source": [
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "# create a module to normalize input image so we can easily put it in a\n",
        "# ``nn.Sequential``\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
        "        # directly work with image Tensor of shape [B x C x H x W].\n",
        "        # B is batch size. C is number of channels. H is height and W is width.\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # normalize ``img``\n",
        "        return (img - self.mean) / self.std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C8Vo7oFxq7l"
      },
      "source": [
        "Y creamos el modelo para evaluar la pérdida en contenido y en estilo.\n",
        "\n",
        "- Para contenido vamos a usar solo la capa convolucional 4\n",
        "- Para estilo vamos a usar las capas convolucionales 1, 2, 3, 5 y 5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jG0TptVLzQN-"
      },
      "outputs": [],
      "source": [
        "content_layers_default = ['conv_4']\n",
        "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hXVCWMlzI1N"
      },
      "source": [
        "Vamos a crear una función que devuelva 3 objetos:\n",
        "\n",
        "1. El modelo para evaluar una imagen objetivo\n",
        "2. La funcion de pérdida de contenido para una imagen de contenido particular\n",
        "3. La funcion de pérdida de estilo para una imagen de estilo particular\n",
        "\n",
        "Esta función esta complicada y es el corazón del modelo, hay qye verla con cuidado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-D8TG0G2zIo0"
      },
      "outputs": [],
      "source": [
        "def get_style_model_and_losses(\n",
        "    cnn,\n",
        "    normalization_mean, normalization_std,\n",
        "    style_img,\n",
        "    content_img,\n",
        "    content_layers=content_layers_default,\n",
        "    style_layers=style_layers_default\n",
        "    ):\n",
        "\n",
        "    # normalization module\n",
        "    normalization = Normalization(normalization_mean, normalization_std)\n",
        "\n",
        "    # just in order to have an iterable access to content/style losses\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    # assuming that ``cnn`` is a ``nn.Sequential``, so we make a new ``nn.Sequential``\n",
        "    # to put in modules that are supposed to be activated sequentially\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0  # increment every time we see a conv\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = f'conv_{i}'\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f'relu_{i}'\n",
        "            # The in-place version doesn't play very nicely with the ``ContentLoss``\n",
        "            # and ``StyleLoss`` we insert below. So we replace with out-of-place\n",
        "            # ones here.\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = f'pool_{i}'\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = f'bn_{i}'\n",
        "        else:\n",
        "            raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            # add content loss:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            # add style loss:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    # now we trim off the layers after the last content and style losses\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "\n",
        "    model = model[:(i + 1)]\n",
        "\n",
        "    return model, style_losses, content_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCC7eNWv1TMi"
      },
      "source": [
        "## 4. Inicializando el modelo objetivo\n",
        "\n",
        "Empezamos con el mismo de contenido y de ahí modificamos, para no tardar tanto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "llWG-_QFzDIa",
        "outputId": "ace52a6b-8c1d-4f47-c357-b1fc456e8138"
      },
      "outputs": [],
      "source": [
        "input_img = content_img.clone()\n",
        "# Si quieres empezar con una imagen de ruido blanco usa esto:\n",
        "# input_img = torch.randn(content_img.data.size())\n",
        "\n",
        "plt.figure()\n",
        "imshow(input_img, title='Inicial target Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIxam-Up13pp"
      },
      "source": [
        "## 5. Funciones de entrenamiento (ajuste de imagen)\n",
        "\n",
        "Vamos a usar un método muy eficiente aunque costoso de optimización, el LBFGS (para mayor explicacion del método en [este enlace de wikipedia](https://en.wikipedia.org/wiki/Limited-memory_BFGS))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GsxsEAZw19nX"
      },
      "outputs": [],
      "source": [
        "def get_input_optimizer(input_img):\n",
        "    # this line to show that input is a parameter that requires a gradient\n",
        "    optimizer = optim.LBFGS([input_img])\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkdC6Z9x2s20"
      },
      "source": [
        "Y ahora si el chuqui de todo este asunto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dh4LD29O2w1V"
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(\n",
        "    cnn,\n",
        "    normalization_mean, normalization_std,\n",
        "    content_img,\n",
        "    style_img,\n",
        "    input_img,\n",
        "    num_steps=300,\n",
        "    style_weight=1000000,\n",
        "    content_weight=1\n",
        "    ):\n",
        "\n",
        "    \"\"\"Run the style transfer.\"\"\"\n",
        "    print('Building the style transfer model..')\n",
        "\n",
        "    model, style_losses, content_losses = get_style_model_and_losses(\n",
        "        cnn,\n",
        "        normalization_mean, normalization_std,\n",
        "        style_img,\n",
        "        content_img\n",
        "    )\n",
        "\n",
        "    # We want to optimize the input and not the model parameters so we\n",
        "    # update all the requires_grad fields accordingly\n",
        "    input_img.requires_grad_(True)\n",
        "\n",
        "    # We also put the model in evaluation mode, so that specific layers\n",
        "    # such as dropout or batch normalization layers behave correctly.\n",
        "    model.eval()\n",
        "    model.requires_grad_(False)\n",
        "\n",
        "    optimizer = get_input_optimizer(input_img)\n",
        "\n",
        "    print('Optimizando..')\n",
        "    run = [0]\n",
        "    while run[0] <= num_steps:\n",
        "\n",
        "        def closure():\n",
        "            # correct the values of updated input image\n",
        "            with torch.no_grad():\n",
        "                input_img.clamp_(0, 1)\n",
        "\n",
        "            # Gradientes a 0\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Calcula los gradientes y los errores de cada capa\n",
        "            model(input_img)\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "\n",
        "            # Agrega las pérdidas de estilo y de contenido por capas\n",
        "            for sl in style_losses:\n",
        "                style_score += sl.loss\n",
        "            for cl in content_losses:\n",
        "                content_score += cl.loss\n",
        "\n",
        "            # Importancia de contenido y estilo\n",
        "            style_score *= style_weight\n",
        "            content_score *= content_weight\n",
        "\n",
        "            # Calcula la pérdida conjunta\n",
        "            loss = style_score + content_score\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            # Cada 50 epochs\n",
        "            if run[0] % 50 == 0:\n",
        "                print(\"run {}:\".format(run))\n",
        "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
        "                    style_score.item(), content_score.item()))\n",
        "                print()\n",
        "\n",
        "            return style_score + content_score\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    # a last correction...\n",
        "    with torch.no_grad():\n",
        "        input_img.clamp_(0, 1)\n",
        "\n",
        "    return input_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hkjz3aj4MgC"
      },
      "source": [
        "## 6. Ejecutando el algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoSWYea74SN4",
        "outputId": "8c4093b2-b8d8-4d91-ae61-59f31d3d3461"
      },
      "outputs": [],
      "source": [
        "input_img = content_img.clone()\n",
        "output = run_style_transfer(\n",
        "    cnn,\n",
        "    cnn_normalization_mean, cnn_normalization_std,\n",
        "    content_img,\n",
        "    style_img,\n",
        "    input_img,\n",
        "    num_steps=300,\n",
        "    style_weight=1000000,\n",
        "    content_weight=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "0p3B-PBL4zoT",
        "outputId": "fea16a17-76ea-4082-eeb7-5b7570c36282"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 30))\n",
        "plt.subplot(1,3,1)\n",
        "imshow(content_img, title='Content Image')\n",
        "plt.subplot(1,3,2)\n",
        "imshow(style_img, title='Style Image')\n",
        "plt.subplot(1,3,3)\n",
        "imshow(output, title='Output Image')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
